```{r}
# load libraries
library(dplyr)
library(tidyverse)
library(glmnet)
library(caret)
library(randomForest)
library(DataExplorer)
library(leaps)
library(MASS)
```

```{r}
# read in data
#data = read.csv("https://www.dropbox.com/scl/fi/0yl5e0uwhh2mypl2htt7h/01_2022_HMDA_Filter.csv?rlkey=1z1qtm6h2h8rr1ubauk943la5&dl=1")
data = read.csv("01_2022_HMDA_Filter.csv")
head(data)
```

```{r}
# add column for loan outcome (positive outcome = 1, 2, 8 and negative outcome = 3, 7)
data = data %>%
  mutate(
    outcome = case_when(
      action_taken %in% c(1, 2, 8) ~ 1,
      action_taken %in% c(3, 7) ~ 0,
    )
  )

# drop rows without an outcome
data = data %>% drop_na(outcome)

# filter to home purchase loans
data = data %>%
  filter(loan_purpose == 1
  )
```

```{r}
data_copy = data

# drop a subset of columns (add back activity_year later)
drop_columns = c('activity_year', 'loan_purpose', 'lei', 'derived_msa_md', 'county_code', 'census_tract', 'action_taken', 'total_points_and_fees', 'discount_points', 'lender_credits', 'prepayment_penalty_term', 'intro_rate_period', 'multifamily_affordable_units', 'co_applicant_credit_score_type', 'applicant_ethnicity_1', 'applicant_ethnicity_2', 'applicant_ethnicity_3', 'applicant_ethnicity_4', 'applicant_ethnicity_5', 'co_applicant_ethnicity_1', 'co_applicant_ethnicity_2', 'co_applicant_ethnicity_3', 'co_applicant_ethnicity_4', 'co_applicant_ethnicity_5', 'applicant_ethnicity_observed', 'co_applicant_ethnicity_observed', 'applicant_race_1', 'applicant_race_2', 'applicant_race_3', 'applicant_race_4', 'applicant_race_5', 'co_applicant_race_1', 'co_applicant_race_2', 'co_applicant_race_3', 'co_applicant_race_4', 'co_applicant_race_5', 'applicant_race_observed', 'co_applicant_race_observed', 'applicant_sex', 'co_applicant_sex', 'co_applicant_age', 'applicant_sex_observed', 'co_applicant_sex_observed', 'co_applicant_age_above_62', 'aus_1', 'aus_2', 'aus_3', 'aus_4', 'aus_5', 'denial_reason_1', 'denial_reason_2', 'denial_reason_3', 'denial_reason_4', 'interest_rate', 'rate_spread', 'total_loan_costs', 'origination_charges', 'combined_loan_to_value_ratio', 'reverse_mortgage')
data_reduced = data_copy[ , !(names(data_copy) %in% drop_columns)]

# change the type of specified columns
int_columns = data_reduced %>% dplyr::select(where(is.integer))
data_reduced = mutate_at(data_reduced, vars(names(int_columns)), as.character)
dbl_columns = data_reduced %>% dplyr::select(where(is.double))
data_reduced = mutate_at(data_reduced, vars(names(dbl_columns)), as.character)
numeric_columns = c('loan_amount', 'loan_term', 'property_value', 'total_units', 'income', 'tract_population', 'tract_minority_population_percent', 'ffiec_msa_md_median_family_income', 'tract_to_msa_income_percentage', 'tract_owner_occupied_units', 'tract_one_to_four_family_homes', 'tract_median_age_of_housing_units')
data_reduced = mutate_at(data_reduced, vars(numeric_columns), as.numeric)

# other data type casting
#data_copy$applicant_credit_score_type = as.character(data_copy$applicant_credit_score_type)

# convert categorical variables to factors
data_reduced[sapply(data_reduced, is.character)] = lapply(data_reduced[sapply(data_reduced, is.character)], as.factor)

# center and scale numeric features
#data_reduced = data_reduced %>% mutate_if(is.numeric, scale)

# omit rows with missing data
data_reduced = na.omit(data_reduced)

head(data_reduced)
```

```{r}
# Define the control using a random forest selection function
control = rfeControl(functions = rfFuncs, # random forest
                      method = "repeatedcv", # repeated cv
                      repeats = 5, # number of repeats
                      number = 10) # number of folds

# Features
x = data_reduced %>%
  select(-outcome) %>%
  as.data.frame()

# Target variable
y = data_reduced$outcome

# Training: 80%; Test: 20%
set.seed(123)
inTrain = createDataPartition(y, p = .80, list = FALSE)[,1]

x_train = x[ inTrain, ]
x_test = x[-inTrain, ]

y_train = y[ inTrain]
y_test = y[-inTrain]
```

```{r}
# Run RFE for feature selection
result_rfe1 = rfe(x = x_train, 
                   y = y_train, 
                   sizes = c(1:10, 20, 30),
                   rfeControl = control)

# Print the results
result_rfe1

# Print the selected features
predictors(result_rfe1)

# Print the results visually
ggplot(data = result_rfe1, metric = "Accuracy") + theme_bw()
ggplot(data = result_rfe1, metric = "Kappa") + theme_bw()
```

```{r}
# split by state
TX_2022_df = data_reduced[data_reduced$state_code == 'TX',][-1]
NY_2022_df = data_reduced[data_reduced$state_code == 'NY',][-1]
WA_2022_df = data_reduced[data_reduced$state_code == 'WA',][-1]
head(WA_2022_df)
#sapply(WA_2022_df, function(x) length(unique(x)))
```

```{r}
columns_of_interest = c('outcome', 'derived_ethnicity', 'derived_race', 'derived_sex', 'loan_type', 'business_or_commercial_purpose', 'loan_amount', 'loan_term', 'income', 'occupancy_type', 'applicant_age')
WA_2022_df_reduced = WA_2022_df[ , (names(WA_2022_df) %in% columns_of_interest)]
head(WA_2022_df_reduced)
```

```{r}
# build logistic regression model
model_WA_2022 = glm(outcome ~.,family=binomial(link='logit'),data=WA_2022_df_reduced)
summary(model_WA_2022)
```

```{r}
# Find the best lambda using cross-validation
set.seed(123) 
x = WA_2022_df_reduced[, colnames(WA_2022_df_reduced)[colnames(WA_2022_df_reduced) != 'outcome']] 
x = model.matrix( ~ ., x)
y = WA_2022_df_reduced$outcome

cv.lasso = cv.glmnet(x, y, alpha = 1, family = "binomial")
# Fit the final model on the training data
model = glmnet(x, y, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)
#coef(model)
```

```{r}
# Display regression coefficients
plot(cv.lasso)
cv.lasso$lambda.min
cv.lasso$lambda.1se
coef(cv.lasso, cv.lasso$lambda.min)
coef(cv.lasso, cv.lasso$lambda.1se)
```

```{r}
# Split the data into training and test set
set.seed(123)
sample = sample(c(TRUE, FALSE), nrow(WA_2022_df_reduced), replace=TRUE, prob=c(0.7,0.3))
train = WA_2022_df_reduced[sample, ]
test = WA_2022_df_reduced[!sample, ]
```

```{r}
# Make predictions on the test data
x.test = model.matrix(outcome ~., test)#[,-1]
probabilities = predict(model, s=c("lambda.1se", "lambda.min"), newx=x.test)
predicted.classes = ifelse(probabilities > 0.5, "pos", "neg")

# Model accuracy
observed.classes = test$outcome
mean(predicted.classes == observed.classes)
```

```{r}
# step-wise regression full model 
full.model <- glm(outcome ~., family='binomial', data = WA_2022_df_reduced)
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "both", 
                      trace = TRUE)
summary(step.model)
```

```{r}
models = regsubsets(outcome~., data = WA_2022_df_reduced, nvmax = 3,
                     method = "seqrep")
summary(models)
```
